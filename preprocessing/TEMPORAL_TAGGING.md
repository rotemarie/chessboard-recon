

# Temporal Frame Tagging for PGN Videos

This module uses **optical flow analysis** and **model-based validation** to automatically tag chess video frames with correct FEN positions.

## ðŸŽ¯ Problem Statement

**Regular games (game2-7):** Manually curated CSV files mapping specific frames to verified FEN positions.

**PGN games (game8-13):** Only have PGN move sequences + raw video frames. Naive even-sampling doesn't work because:
- Players think for varying amounts of time (30s vs 2s per move)
- Transition frames show pieces in air or hands blocking the board
- This causes severe mislabeling (empty squares tagged as queens, etc.)

## ðŸ”¬ Our Solution: Temporal Analysis

### Step 1: Stability Detection (Optical Flow)
Use optical flow to detect when the board is actually stable:

```python
# Compute optical flow between consecutive frames
flow = cv2.calcOpticalFlowFarneback(frame1, frame2, ...)
magnitude = np.mean(np.sqrt(flow[..., 0]**2 + flow[..., 1]**2))

# Low flow = stable board, High flow = piece moving
is_stable = magnitude < threshold
```

**Output:** Segments of stable frames (e.g., frames 100-250, 500-680, ...)

### Step 2: Chronological Alignment
Align stable segments to PGN positions in order:

```
Stable Segment 1 â†’ PGN Position 0
Stable Segment 2 â†’ PGN Position 1
Stable Segment 3 â†’ PGN Position 2
...
```

### Step 3: Model Validation
Use trained classifier to verify each alignment:

```python
predicted_fen = classify_board(frame)
expected_fen = pgn_positions[idx]

similarity = count_matching_squares(predicted_fen, expected_fen) / 64

if similarity > 0.80:  # 80% match required
    accept_assignment()
else:
    try_next_position()  # Might have missed a position
```

## ðŸ“Š Expected Performance

Based on temporal properties of chess videos:

| Metric | Expected | Notes |
|--------|----------|-------|
| **Segment Detection** | 90%+ | Most moves create clear stable periods |
| **Alignment Accuracy** | 70-85% | With 80% similarity threshold |
| **False Positives** | <5% | Model validation filters bad matches |

**Comparison to naive sampling:** 30-40% accuracy â†’ **70-85% accuracy** âœ¨

## ðŸš€ Usage

### Single Game

```bash
python temporal_tagger.py \
  --video data/PGN/c06/game8/video.mp4 \
  --pgn data/PGN/c06/game8/game8.pgn \
  --model ../model/resnet18_ft.pth \
  --classes ../model/classes.txt \
  --output pgn_output/game8_tagged.csv \
  --min-similarity 0.80
```

**Output CSV:**
```
frame_idx,fen,similarity
245,rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR,0.9531
1420,rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR,0.8906
2560,rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR,0.8750
...
```

### Batch Processing

```bash
python process_pgn_temporal.py \
  --pgn-root ../data/PGN \
  --model ../model/resnet18_ft.pth \
  --classes ../model/classes.txt \
  --output ../pgn_temporal_output \
  --min-similarity 0.80 \
  --save-frames
```

**Options:**
- `--games game8 game9` - Process specific games only
- `--flow-threshold 25` - Percentile for stability (lower = stricter)
- `--min-stable-frames 15` - Minimum frames for a stable segment
- `--min-similarity 0.80` - Minimum FEN match required (0-1)
- `--save-frames` - Save sample frames for inspection

## ðŸ“ Output Structure

```
pgn_temporal_output/
â”œâ”€â”€ tagged_csvs/
â”‚   â”œâ”€â”€ game8_tagged.csv
â”‚   â”œâ”€â”€ game9_tagged.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tagged_frames/         # If --save-frames
â”‚   â”œâ”€â”€ game8/
â”‚   â”‚   â”œâ”€â”€ frame_000245.jpg
â”‚   â”‚   â”œâ”€â”€ frame_001420.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ game9/
â””â”€â”€ temp_videos/           # Created if needed
    â””â”€â”€ game8.mp4
```

## ðŸ”§ Parameters Tuning

### Flow Threshold (`--flow-threshold`)
- **Lower (15-20):** Stricter stability, fewer segments, higher quality
- **Higher (30-40):** More permissive, more segments, might include transitions
- **Default: 25** (good balance)

### Minimum Stable Frames (`--min-stable-frames`)
- **Lower (5-10):** Accept shorter stable periods (useful for blitz games)
- **Higher (20-30):** Only accept long stable periods (safer)
- **Default: 15** (~0.5 seconds at 30fps)

### Similarity Threshold (`--min-similarity`)
- **Lower (0.70):** More permissive, more alignments, some errors
- **Higher (0.90):** Very strict, fewer alignments, high confidence
- **Default: 0.80** (51/64 squares must match)

## ðŸ§ª Validation

After processing, inspect results:

```bash
# View sample frames for game8
ls pgn_temporal_output/tagged_frames/game8/

# Check alignment statistics
cat pgn_temporal_output/tagged_csvs/game8_tagged.csv | wc -l
# Compare to: cat data/PGN/c06/game8/game8.pgn (count moves)
```

**Good alignment:**
- Coverage: >70% of PGN positions tagged
- Similarity: Average >85%

**Poor alignment:**
- Coverage: <50% (video quality issues or model failures)
- Similarity: <80% (wrong game phase or setup)

## ðŸ” Troubleshooting

### "No alignments found"
- Check if board is consistently visible in video
- Lower `--min-similarity` to 0.70
- Check model quality on this specific game's board/pieces

### "Too few segments detected"
- Increase `--flow-threshold` to 35-40
- Decrease `--min-stable-frames` to 10
- Video might have very stable camera (less motion blur)

### "Alignments skip positions"
- Normal! Some positions might not have stable frames (quick moves)
- Model might fail on unusual board angles
- Check tagged frames to verify actual issues

## ðŸ“ˆ Integration with Training Pipeline

Once you have tagged CSVs:

```bash
# 1. Use tagged CSVs to extract and label squares
python preprocess_from_temporal_tags.py \
  --tagged-dir pgn_temporal_output/tagged_csvs \
  --videos-dir data/PGN \
  --output preprocessed_data_pgn

# 2. Combine with original dataset
python split_dataset.py \
  --preprocessed-root preprocessed_data_combined \
  --output-root dataset_combined

# 3. Train on expanded dataset
cd ../training
python train.py --data-dir ../dataset_combined
```

## ðŸŽ“ Technical Details

### Why Optical Flow?

Optical flow captures pixel-level motion between frames. Chess moves create distinctive patterns:
- **High flow:** Hand enters frame, piece lifted, piece moves
- **Low flow:** Board at rest between moves

This is more reliable than:
- Image similarity (fails with lighting changes)
- Edge detection (fails with shadows)
- Time-based sampling (wrong assumption)

### Why Model Validation?

Even with good stability detection, segment-to-position alignment needs validation because:
- A stable segment might show the wrong position (camera moved during transition)
- Multiple stable segments might show the same position (player thinking)
- The model acts as a "ground truth" oracle to verify assignments

### Bidirectional LSTM (Optional Future Enhancement)

The current approach is unidirectional (process segments in order). A bidirectional LSTM could:
- Look ahead/behind to resolve ambiguous alignments
- Detect missed positions by finding gaps in the sequence
- Improve coverage from 70-85% â†’ 85-95%

## ðŸ¤ Contributing

To improve this system:
1. **Better stability detection:** Try perceptual hashing, SSIM, or learned features
2. **Smarter alignment:** Dynamic programming to find optimal sequence alignment
3. **Active learning:** Flag uncertain assignments for manual review
4. **Multi-angle support:** Handle games shot from different perspectives

## ðŸ“š References

- Farneback, G. (2003). "Two-Frame Motion Estimation Based on Polynomial Expansion"
- Optical Flow: [OpenCV docs](https://docs.opencv.org/master/d4/dee/tutorial_optical_flow.html)
- FEN Notation: [Wikipedia](https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation)
